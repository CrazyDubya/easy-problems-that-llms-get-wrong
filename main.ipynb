{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from charting import create_performance_chart\n",
    "from llm_service import litellm_service, custom_llm_service\n",
    "from utils import get_llm_answers, get_llm_stats, load_all_llm_answers_from_json, model_clean\n",
    "from auto_eval import create_all_llm_eval_messages, extract_all_scores, create_auto_eval_json, get_llm_eval_responses\n",
    "\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_models = [\n",
    "    \"gpt-4-turbo-preview\", \n",
    "    \"meta.llama3-70b-instruct-v1:0\", \n",
    "    \"mistral/mistral-large-latest\", \n",
    "    \"mistral/open-mixtral-8x22b\", \n",
    "    \"claude-3-opus-20240229\", \n",
    "    \"vertex_ai/gemini-1.5-pro\", \n",
    "    \"vertex_ai/gemini-1.0-pro\",\n",
    "    \"command-r\", \n",
    "]\n",
    "\n",
    "answer_hyperparams = {\n",
    "    'batch_size': 10, # Max number of questions to send to a model at once (10 is sensible)\n",
    "    'temperature': 0, # 0 is default and the most deterministic\n",
    "    'max_tokens': 2048, # 2048 works for most models, but may have to be reduced for some models\n",
    "}\n",
    "\n",
    "\n",
    "auto_eval_model = \"gpt-4-turbo-preview\"\n",
    "auto_eval_hyperparams= {\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 2048,\n",
    "    'batch_size': 30,\n",
    "}\n",
    "\n",
    "\n",
    "date_now = datetime.now().strftime('%Y-%m-%d')\n",
    "answers_save_path = f\"./{date_now}-Benchmark/llm_outputs\"\n",
    "auto_eval_save_path = f\"./{date_now}-Benchmark/auto_eval_outputs\"\n",
    "stats_save_path = f\"./{date_now}-Benchmark/tables_and_charts\"\n",
    "\n",
    "\n",
    "execution_steps = [\n",
    "    \"get_llm_answers\",\n",
    "    \"auto_evaluate_answers\",\n",
    "    \"generate_statistics\", \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing LLM answers (in 2024-06-01 folder): ['command-r', 'gemini-1_0-pro', 'meta_llama3-70b-instruct-v1_0', 'mistral-large-latest', 'open-mixtral-8x22b']\n",
      "Skipping existing LLM auto evals (in 2024-06-01 folder): ['open-mixtral-8x22b', 'meta_llama3-70b-instruct-v1_0', 'mistral-large-latest', 'command-r', 'gemini-1_0-pro']\n",
      "----------------------\n",
      "\n",
      "1. GETTING LLM ANSWERS\n",
      "Running  Benchmark for gpt-4-turbo-preview\n",
      "Processing batch 1-10 ex 30\n",
      "Processing batch 11-20 ex 30\n",
      "Processing batch 21-30 ex 30\n",
      "Running  Benchmark for claude-3-opus-20240229\n",
      "Processing batch 1-10 ex 30\n",
      "Processing batch 11-20 ex 30\n",
      "Processing batch 21-30 ex 30\n",
      "Running  Benchmark for vertex_ai/gemini-1.5-pro\n",
      "Processing batch 1-10 ex 30\n",
      "Processing batch 11-20 ex 30\n",
      "Processing batch 21-30 ex 30\n",
      "-- DONE ANSWERS --\n",
      "\n",
      "2. AUTO EVALUATING ANSWERS\n",
      "Running gpt-4-turbo-preview evaluation...\n",
      "Processing batch 1-30 ex 30\n",
      "Running claude-3-opus-20240229 evaluation...\n",
      "Processing batch 1-30 ex 30\n",
      "Running vertex_ai/gemini-1.5-pro evaluation...\n",
      "Processing batch 1-30 ex 30\n",
      "Auto Eval ->> Model: gpt-4-turbo-preview | Mean score: 64.66666666666667 | Std dev: 37.75906681098967\n",
      "Auto Eval ->> Model: claude-3-opus-20240229 | Mean score: 63.0 | Std dev: 38.34057902536163\n",
      "Auto Eval ->> Model: vertex_ai/gemini-1.5-pro | Mean score: 58.666666666666664 | Std dev: 39.977004884561204\n",
      "-- DONE AUTO EVAL --\n",
      "\n",
      "3. GENERATING STATISTICS\n",
      "Calculating stats for claude-3-opus-20240229\n",
      "Calculating stats for command-r\n",
      "Calculating stats for gemini-1_0-pro\n",
      "Calculating stats for gemini-1_5-pro\n",
      "Calculating stats for gpt-4-turbo-preview\n",
      "Calculating stats for meta_llama3-70b-instruct-v1_0\n",
      "Calculating stats for mistral-large-latest\n",
      "Calculating stats for open-mixtral-8x22b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_score</th>\n",
       "      <th>std_dev_score</th>\n",
       "      <th>z_interval_error</th>\n",
       "      <th>ci_lower</th>\n",
       "      <th>ci_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo-preview</th>\n",
       "      <td>65.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-opus-20240229</th>\n",
       "      <td>63.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1_5-pro</th>\n",
       "      <td>59.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open-mixtral-8x22b</th>\n",
       "      <td>51.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta_llama3-70b-instruct-v1_0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-large-latest</th>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command-r</th>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1_0-pro</th>\n",
       "      <td>41.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mean_score  std_dev_score  z_interval_error  \\\n",
       "model                                                                        \n",
       "gpt-4-turbo-preview                  65.0           38.0              14.0   \n",
       "claude-3-opus-20240229               63.0           38.0              14.0   \n",
       "gemini-1_5-pro                       59.0           40.0              14.0   \n",
       "open-mixtral-8x22b                   51.0           33.0              12.0   \n",
       "meta_llama3-70b-instruct-v1_0        50.0           38.0              14.0   \n",
       "mistral-large-latest                 47.0           43.0              15.0   \n",
       "command-r                            42.0           43.0              15.0   \n",
       "gemini-1_0-pro                       41.0           38.0              14.0   \n",
       "\n",
       "                               ci_lower  ci_upper  \n",
       "model                                              \n",
       "gpt-4-turbo-preview                51.0      77.0  \n",
       "claude-3-opus-20240229             49.0      76.0  \n",
       "gemini-1_5-pro                     45.0      73.0  \n",
       "open-mixtral-8x22b                 40.0      63.0  \n",
       "meta_llama3-70b-instruct-v1_0      37.0      64.0  \n",
       "mistral-large-latest               33.0      62.0  \n",
       "command-r                          27.0      57.0  \n",
       "gemini-1_0-pro                     27.0      55.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DONE STATS --\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in benchmark questions\n",
    "benchmark_questions = json.load(open('linguistic_benchmark.json', 'r'))\n",
    "# Load in any existing answers and evals to avoid overwriting them\n",
    "all_llm_answers = load_all_llm_answers_from_json(answers_save_path, prefix_replace='final_answers-')\n",
    "all_llm_evals = load_all_llm_answers_from_json(auto_eval_save_path, prefix_replace='auto_eval-')\n",
    "skip_evals = set(all_llm_evals.keys() & set(all_llm_answers.keys()))\n",
    "print(f'Skipping existing LLM answers (in {date_now} folder):', list(all_llm_answers.keys()))\n",
    "print(f'Skipping existing LLM auto evals (in {date_now} folder):', list(skip_evals))\n",
    "print('----------------------\\n')\n",
    "\n",
    "\n",
    "if \"get_llm_answers\" in execution_steps:\n",
    "    print('1. GETTING LLM ANSWERS')\n",
    "    answer_models_run = [model for model in answer_models \n",
    "                         if model_clean(model) not in all_llm_answers.keys()]\n",
    "    all_llm_answers = await get_llm_answers(\n",
    "        litellm_service(), \n",
    "        benchmark_questions, \n",
    "        answer_models_run, \n",
    "        answer_hyperparams, \n",
    "        answers_save_path,\n",
    "    )\n",
    "    print('-- DONE ANSWERS --\\n')\n",
    "\n",
    "\n",
    "if \"auto_evaluate_answers\" in execution_steps:\n",
    "    print('2. AUTO EVALUATING ANSWERS')\n",
    "    all_llm_answers = {model: value for model, value in all_llm_answers.items() \n",
    "                       if model_clean(model) not in skip_evals}\n",
    "    all_llm_eval_messages = create_all_llm_eval_messages(all_llm_answers, benchmark_questions)\n",
    "    custom_llm = custom_llm_service()\n",
    "    all_llm_eval_responses = await get_llm_eval_responses(\n",
    "        custom_llm, \n",
    "        all_llm_eval_messages, \n",
    "        model=auto_eval_model, \n",
    "        hyperparams=auto_eval_hyperparams,\n",
    "    )\n",
    "    all_llm_scores = extract_all_scores(all_llm_eval_responses)\n",
    "    all_auto_results = create_auto_eval_json(\n",
    "        all_llm_scores, \n",
    "        all_llm_eval_responses, \n",
    "        all_llm_answers, \n",
    "        benchmark_questions, \n",
    "        auto_eval_save_path\n",
    "    )\n",
    "    print('-- DONE AUTO EVAL --\\n')\n",
    "\n",
    "\n",
    "if \"generate_statistics\" in execution_steps:\n",
    "    print('3. GENERATING STATISTICS')\n",
    "    all_llm_evals = load_all_llm_answers_from_json(auto_eval_save_path, prefix_replace='auto_eval-')\n",
    "    stats_df = get_llm_stats(all_llm_evals, stats_save_path, bootstrap_n=10000)\n",
    "    display(stats_df)\n",
    "    plt, barplot = create_performance_chart(stats_df)\n",
    "    barplot.figure.savefig(\"performance_chart.png\")\n",
    "    plt.show()\n",
    "    print('-- DONE STATS --\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
