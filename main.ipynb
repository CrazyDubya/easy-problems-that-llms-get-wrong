{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from charting import create_performance_chart\n",
    "from utils import get_llm_answers, get_llm_stats, load_all_llm_answers_from_json, model_clean\n",
    "from auto_eval import (\n",
    "    create_all_llm_eval_messages, \n",
    "    extract_all_scores, \n",
    "    create_auto_eval_json, \n",
    "    get_llm_eval_responses, \n",
    "    score_multiple_choice_answers,\n",
    "    validation_func,\n",
    "    extract_valid_json,\n",
    ")\n",
    "from multiple_choice import construct_multiple_choice_question\n",
    "from hotz_reflection import construct_hotz_reflection_question\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_models = [\n",
    "    # (\"gpt-4-turbo-preview\", \"litellm\"),\n",
    "    # (\"gpt-4o\", \"litellm\"),\n",
    "    # (\"gpt-4o-mini-2024-07-18\", \"litellm\"),\n",
    "    # (\"o1-preview\", \"custom\"),\n",
    "    # (\"bedrock/meta.llama3-70b-instruct-v1:0\", \"litellm\"),\n",
    "    # (\"Meta-Llama-3-1-70B-Instruct-ostu.eastus.models.ai.azure.com\", \"custom\"),\n",
    "    # (\"Meta-Llama-3-1-405B-Instruct-jjo.eastus.models.ai.azure.com\", \"custom\"),\n",
    "    # (\"mistral-large-latest\", \"custom\"),\n",
    "    # (\"mistral/open-mixtral-8x22b\", \"litellm\"), \n",
    "    # (\"claude-3-opus-20240229\", \"litellm\"), \n",
    "    (\"gemini-1.5-pro\", \"custom\"), \n",
    "    #(\"gemini-1.5-pro-exp-0801\", \"custom\"),\n",
    "    # (\"command-r-plus\", \"litellm\"), \n",
    "    # (\"claude-3-5-sonnet-20240620\", \"litellm\"),\n",
    "]\n",
    "\n",
    "answer_rounds = 10 # Number of rounds of questions to ask each model\n",
    "answer_hyperparams = {\n",
    "    'batch_size': 5, # Max number of questions to send to a model at once (10 is sensible)\n",
    "    'temperature': 0, # 0 is default and the most deterministic\n",
    "    'max_tokens': 2048, # 2048 works for most models, but may have to be reduced for some models\n",
    "    'num_retries': 3, # Number of times to retry a question if it fails\n",
    "}\n",
    "\n",
    "multiple_choice_questions = True\n",
    "\n",
    "if multiple_choice_questions is False:\n",
    "    # Auto evaluation is only supported for open-ended questions and involves an LLM evaluating results against a set of criteria\n",
    "    # Criteria can be found in `auto_eval.py` create_eval_prompt function\n",
    "    auto_eval_rounds = 1 # Number of rounds of auto evaluation to run to then average the scores\n",
    "    auto_eval_model = (\"gpt-4o\", \"custom\"),\n",
    "    auto_eval_hyperparams= {\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 2048,\n",
    "        'batch_size': 30,\n",
    "    }\n",
    "\n",
    "hotz_reflection = False\n",
    "benchmark_name = 'Benchmark' if not multiple_choice_questions else 'Multi-Benchmark'\n",
    "date_now = '2024-07-20' #datetime.now().strftime('%Y-%m-%d') #'2024-07-20'\n",
    "folder_name = f\"{date_now}-{benchmark_name}\" #\"2024-06-21-Multi-Benchmark (temp=0)\" \n",
    "\n",
    "answers_save_path = f\"./{folder_name}/llm_outputs\"\n",
    "answers_hotz_save_path = f\"./{folder_name}/hotz_reflection\"\n",
    "auto_eval_save_path = f\"./{folder_name}/auto_eval_outputs\"\n",
    "auto_eval_hotz_save_path = f\"./{folder_name}/auto_eval_hotz_outputs\"\n",
    "stats_save_path = f\"./{folder_name}/tables_and_charts\"\n",
    "\n",
    "\n",
    "execution_steps = [\n",
    "    \"get_llm_answers\",\n",
    "    # \"hotz_reflection\",\n",
    "    \"evaluate_answers\",\n",
    "    \"generate_statistics\", \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_incomplete_llm_answers(all_llm_answers, auto_eval_save_path, sub_eval_folders, date_now):\n",
    "    all_llm_evals = load_all_llm_answers_from_json(auto_eval_save_path,\n",
    "        prefix_replace='auto_eval-', sub_folders=sub_eval_folders)\n",
    "    skip_evals = set(all_llm_evals.keys() & set(all_llm_answers.keys()))\n",
    "    print(f'Skipping existing LLM evals (in {date_now} folder):', skip_evals)\n",
    "    incomplete_llm_answers = {model: value for model, value in all_llm_answers.items() \n",
    "                               if model_clean(model) not in skip_evals}\n",
    "    return incomplete_llm_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in benchmark questions\n",
    "if multiple_choice_questions:\n",
    "    benchmark_questions = json.load(open(\"linguistic_benchmark_multi_choice.json\", \"r\"))\n",
    "    sub_eval_folders = [\"\"]\n",
    "\n",
    "    def answer_validation_func(x):\n",
    "        return validation_func(\n",
    "            x, json_key=\"ANSWER\", list_of_values=[\"A\", \"B\", \"C\", \"D\"]\n",
    "        )\n",
    "\n",
    "else:\n",
    "    benchmark_questions = json.load(open(\"linguistic_benchmark.json\", \"r\"))\n",
    "    sub_eval_folders = [f\"/round_{r+1}\" for r in range(auto_eval_rounds)]\n",
    "sub_answer_folders = [f\"/round_{r+1}\" for r in range(answer_rounds)]\n",
    "\n",
    "\n",
    "if \"get_llm_answers\" in execution_steps:\n",
    "    print(\"1. GETTING LLM ANSWERS\")\n",
    "    # Load in any existing answers and evals to avoid overwriting them\n",
    "    for n in range(answer_rounds):\n",
    "        print(f\"\\n----- Round: {n+1} of {answer_rounds} -----\")\n",
    "        answer_save_path_round = f\"{answers_save_path}/round_{n+1}\"\n",
    "        all_llm_answers = load_all_llm_answers_from_json(\n",
    "            answer_save_path_round, prefix_replace=\"final_answers-\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Skipping existing LLM answers (in {answer_save_path_round} folder):\",\n",
    "            list(all_llm_answers.keys()),\n",
    "        )\n",
    "        answer_models_run = [\n",
    "            model\n",
    "            for model in answer_models\n",
    "            if model_clean(model[0]) not in all_llm_answers.keys()\n",
    "        ]\n",
    "        questions = benchmark_questions.copy()\n",
    "        if multiple_choice_questions:\n",
    "            questions = {}\n",
    "            for model, llm_service in answer_models_run:\n",
    "                clean_model = model_clean(model)\n",
    "                questions[clean_model] = benchmark_questions.copy()\n",
    "                for q in questions[clean_model]:\n",
    "                    prompt, correct_letter = construct_multiple_choice_question(q)\n",
    "                    q.update(\n",
    "                        {\n",
    "                            \"multi_choice_question\": prompt,\n",
    "                            \"correct_letter\": correct_letter,\n",
    "                        }\n",
    "                    )\n",
    "        all_llm_answers = await get_llm_answers(\n",
    "            questions,\n",
    "            answer_models_run,\n",
    "            answer_hyperparams,\n",
    "            answer_save_path_round,\n",
    "            multiple_choice_questions,\n",
    "            validation_func=(\n",
    "                answer_validation_func if multiple_choice_questions else lambda x: True\n",
    "            ),\n",
    "        )\n",
    "    print(\"-- DONE ANSWERS --\\n\")\n",
    "\n",
    "\n",
    "if \"hotz_reflection\" in execution_steps:\n",
    "    print(\"1.5 GETTING HOTZ REFLECTION ANSWERS\")\n",
    "    # Load in any existing answers and evals to avoid overwriting them\n",
    "    for n in range(answer_rounds):\n",
    "        print(f\"\\n----- Round: {n+1} of {answer_rounds} -----\")\n",
    "        answer_save_path_round = f\"{answers_save_path}/round_{n+1}\"\n",
    "        answer_hotz_save_path_round = f\"{answers_hotz_save_path}/round_{n+1}\"\n",
    "        all_llm_answers = load_all_llm_answers_from_json(\n",
    "            answer_save_path_round, prefix_replace=\"final_answers-\"\n",
    "        )\n",
    "        all_hotz_llm_answers = load_all_llm_answers_from_json(\n",
    "            answer_hotz_save_path_round, prefix_replace=\"final_answers-\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Skipping existing LLM answers (in {answer_hotz_save_path_round} folder):\",\n",
    "            list(all_hotz_llm_answers.keys()),\n",
    "        )\n",
    "        answer_models_run = [\n",
    "            model\n",
    "            for model in answer_models\n",
    "            if model_clean(model[0]) not in all_hotz_llm_answers.keys()\n",
    "        ]\n",
    "        all_llm_questions = {\n",
    "            model: info.to_dict(\"records\") for model, info in all_llm_answers.items()\n",
    "        }\n",
    "        if multiple_choice_questions:\n",
    "            for model, questions in all_llm_questions.items():\n",
    "                for q in questions:\n",
    "                    prompt = construct_hotz_reflection_question(q)\n",
    "                    q.update({\"multi_choice_question_pre\": q[\"multi_choice_question\"]})\n",
    "                    q.update({\"model_answer_pre\": q[\"model_answer\"]})\n",
    "                    q.update({\"multi_choice_question\": prompt})\n",
    "\n",
    "        all_llm_answers = await get_llm_answers(\n",
    "            all_llm_questions,\n",
    "            answer_models_run,\n",
    "            answer_hyperparams,\n",
    "            answer_hotz_save_path_round,\n",
    "            multiple_choice_questions,\n",
    "            validation_func=(\n",
    "                answer_validation_func if multiple_choice_questions else lambda x: True\n",
    "            ),\n",
    "        )\n",
    "    print(\"-- DONE ANSWERS --\\n\")\n",
    "\n",
    "\n",
    "if \"evaluate_answers\" in execution_steps:\n",
    "    print(\"2. EVALUATING ANSWERS\")\n",
    "    all_llm_answers = load_all_llm_answers_from_json(\n",
    "        answers_save_path,\n",
    "        prefix_replace=\"final_answers-\",\n",
    "        sub_folders=sub_answer_folders,\n",
    "    )\n",
    "    if multiple_choice_questions:\n",
    "        incomplete_llm_answers = all_llm_answers\n",
    "        all_llm_evals = score_multiple_choice_answers(\n",
    "            incomplete_llm_answers, auto_eval_save_path\n",
    "        )\n",
    "        if hotz_reflection:\n",
    "            print(\"\\nEvaluate Hotz Reflections\")\n",
    "            all_llm_answers = load_all_llm_answers_from_json(\n",
    "                answers_hotz_save_path,\n",
    "                prefix_replace=\"final_answers-\",\n",
    "                sub_folders=sub_answer_folders,\n",
    "            )\n",
    "            incomplete_llm_answers = calc_incomplete_llm_answers(\n",
    "                all_llm_answers, auto_eval_hotz_save_path, sub_eval_folders, date_now\n",
    "            )\n",
    "            all_llm_evals = score_multiple_choice_answers(\n",
    "                incomplete_llm_answers, auto_eval_hotz_save_path\n",
    "            )\n",
    "    else:\n",
    "        all_llm_eval_messages = create_all_llm_eval_messages(\n",
    "            all_llm_answers, benchmark_questions\n",
    "        )\n",
    "        for n in range(auto_eval_rounds):\n",
    "            print(f\"- Round: {n+1} of {auto_eval_rounds} -\")\n",
    "            incomplete_llm_answers = calc_incomplete_llm_answers(\n",
    "                all_llm_eval_messages, auto_eval_save_path, [f\"/round_{n+1}\"], date_now\n",
    "            )\n",
    "            all_llm_eval_responses = await get_llm_eval_responses(\n",
    "                incomplete_llm_answers,\n",
    "                model_info=auto_eval_model,\n",
    "                hyperparams=auto_eval_hyperparams,\n",
    "                validation_func=lambda x: validation_func(\n",
    "                    x, json_key=\"score\", list_of_values=[0, 20, 40, 60, 80, 100]\n",
    "                ),\n",
    "            )\n",
    "            all_llm_scores = extract_all_scores(all_llm_eval_responses)\n",
    "            auto_eval_save_path_n = f\"{auto_eval_save_path}/round_{n+1}\"\n",
    "            all_auto_results = create_auto_eval_json(\n",
    "                all_llm_scores,\n",
    "                all_llm_eval_responses,\n",
    "                all_llm_answers,\n",
    "                benchmark_questions,\n",
    "                auto_eval_save_path_n,\n",
    "            )\n",
    "    print(\"-- DONE EVAL --\\n\")\n",
    "\n",
    "\n",
    "if \"generate_statistics\" in execution_steps:\n",
    "    print(\"3. GENERATING STATISTICS\")\n",
    "    all_stats_dfs = {}\n",
    "    save_info = [\n",
    "        {\n",
    "            \"path\": auto_eval_save_path,\n",
    "            \"chart_title\": \"LLM Linguistic Benchmark Performance\",\n",
    "            \"type\": \"\",\n",
    "        }\n",
    "    ]\n",
    "    if hotz_reflection:\n",
    "        save_info.append(\n",
    "            {\n",
    "                \"path\": auto_eval_hotz_save_path,\n",
    "                \"chart_title\": \"LLM Linguistic Benchmark Performance (Hotz Reflection)\",\n",
    "                \"type\": \"-Hotz\",\n",
    "            }\n",
    "        )\n",
    "    for info in save_info:\n",
    "        save_path = info[\"path\"]\n",
    "        chart_title = info[\"chart_title\"]\n",
    "        info_type = info[\"type\"]\n",
    "        print(\"Eval for path:\", save_path)\n",
    "        all_llm_evals = load_all_llm_answers_from_json(\n",
    "            save_path,\n",
    "            prefix_replace=\"auto_eval-\",\n",
    "            sub_folders=sub_eval_folders,\n",
    "        )\n",
    "        stats_df = get_llm_stats(\n",
    "            all_llm_evals, stats_save_path, file_suffix=info_type, bootstrap_n=10000\n",
    "        )\n",
    "        if multiple_choice_questions:\n",
    "            for model, evals_df in all_llm_evals.items():\n",
    "                # evals_df['invalid_answer_letter'] = evals_df.apply(lambda x: x['json_answer_letter'] not in ['A', 'B', 'C', 'D'], axis=1)\n",
    "                incorrect_letter_count = evals_df[\"invalid_answer_letter\"].sum()\n",
    "                print(model, incorrect_letter_count)\n",
    "                stats_df.loc[model, \"invalid_outputs\"] = incorrect_letter_count\n",
    "\n",
    "        display(stats_df)\n",
    "        barplot, plt = create_performance_chart(\n",
    "            stats_df.reset_index(),\n",
    "            chart_title,\n",
    "            highlight_models=[\"o1-preview\"],\n",
    "        )\n",
    "        barplot.figure.savefig(f\"{stats_save_path}/performance_chart{info_type}.png\")\n",
    "        plt.show()\n",
    "        all_stats_dfs[chart_title] = stats_df\n",
    "    print(\"-- DONE STATS --\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Answers for a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'gpt-4-turbo-preview'\n",
    "# answers = all_llm_evals[model][['question', 'model_answer', 'json_answer', 'correct_letter', 'score']]\n",
    "# for row in answers.to_dict('records'):\n",
    "#     print('--------------')\n",
    "#     print('\\nQuestion\\n', row['question'])\n",
    "#     print('\\nModel Answer\\n', row['model_answer'])\n",
    "#     print('\\nJson Answer\\n', row['json_answer'])\n",
    "#     print('\\nCorrect Letter\\n', row['correct_letter'])    \n",
    "#     print('\\nScore\\n', row['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Hotz Reflections Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_llm_evals = load_all_llm_answers_from_json(\n",
    "    auto_eval_hotz_save_path, \n",
    "    prefix_replace='auto_eval-',\n",
    "    sub_folders=sub_eval_folders,\n",
    ")\n",
    "\n",
    "answer_change_stats = {}\n",
    "for model, model_evals in all_llm_evals.items():\n",
    "    model_evals['json_answer_letter_pre'] = model_evals['model_answer_pre'].map(extract_valid_json).map(lambda x: None if x is None else x['ANSWER'])\n",
    "\n",
    "    compare_letters = model_evals[['json_answer_letter', 'json_answer_letter_pre', 'correct_letter']].copy()\n",
    "    compare_letters['changed_answer'] = compare_letters['json_answer_letter'] != compare_letters['json_answer_letter_pre']\n",
    "    compare_letters['changed_answer_perc'] = (compare_letters['changed_answer'] / len(compare_letters)) * 100\n",
    "    compare_letters['correct'] = compare_letters['json_answer_letter'] == compare_letters['correct_letter']\n",
    "    compare_letters['correct_pre'] = compare_letters['json_answer_letter_pre'] == compare_letters['correct_letter']\n",
    "\n",
    "    compare_letters[['correct', 'correct_pre', 'changed_answer_perc']].sum()\n",
    "    answer_change_stats[model] = compare_letters[['correct_pre', 'correct', 'changed_answer_perc']].sum()\n",
    "final_answer_change_df = pd.DataFrame(answer_change_stats).transpose().sort_values('correct', ascending=False)\n",
    "final_answer_change_df.rename(columns={\n",
    "    'correct_pre': 'Correct (pre-reflection)', \n",
    "    'correct': 'Correct (post-reflection)', \n",
    "    }, inplace=True)\n",
    "final_answer_change_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotz_ref = 'LLM Liguisitc Benchmark Performance (Hotz Reflection)'\n",
    "ref = 'LLM Liguisitc Benchmark Performance'\n",
    "\n",
    "all_stats_dfs[hotz_ref]['mean_score_pre'] = all_stats_dfs[ref]['mean_score']\n",
    "\n",
    "all_stats_dfs[hotz_ref]['diff'] = all_stats_dfs[hotz_ref]['mean_score'] - all_stats_dfs[hotz_ref]['mean_score_pre']\n",
    "all_stats_dfs[hotz_ref]\n",
    "diff_from_pre = all_stats_dfs[hotz_ref][['mean_score', 'mean_score_pre', 'diff']]\n",
    "diff_from_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_from_pre_final_raw = diff_from_pre.merge(final_answer_change_df[['changed_answer_perc']], left_index=True, right_index=True)\n",
    "diff_from_pre_final = diff_from_pre_final_raw[['mean_score_pre', 'mean_score', 'diff', 'changed_answer_perc']].round(0)\n",
    "diff_from_pre_final.rename(columns={\n",
    "    \"mean_score_pre\": \"Pre-Reflection Score\",\n",
    "    \"mean_score\": \"Post-Reflection Score\",\n",
    "    \"diff\": \"Difference\",\n",
    "    \"changed_answer_perc\": \"Answers Changed (%)\",\n",
    "}, inplace=True)\n",
    "#diff_from_pre_final = diff_from_pre_final.astype(int).astype(str) + '%'\n",
    "diff_from_pre_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_from_pre_final['Answers Changed (%)'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Auto Eval Consistancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_llm_evals = load_all_llm_answers_from_json(\n",
    "    auto_eval_save_path, \n",
    "    prefix_replace='auto_eval-',\n",
    "    sub_folders=sub_eval_folders,\n",
    ")\n",
    "models = list(all_llm_evals.keys())\n",
    "\n",
    "\n",
    "model = models[3]\n",
    "print(f\"Model: {model}\")\n",
    "auto_eval_agg = all_llm_evals[model].set_index('level_0').groupby('index').agg({'score': ['mean', 'min', 'max']})\n",
    "auto_eval_agg.index.name = 'Question #'\n",
    "auto_eval_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_folder = \"2024-06-12-Multi-Benchmark (temp=1)/tables_and_charts\"\n",
    "# stats_df = pd.read_csv(f\"{tables_folder}/final_stats.csv\", index_col=0)\n",
    "# barplot, plt = create_performance_chart(stats_df.reset_index())\n",
    "# barplot.figure.savefig(f\"{tables_folder}/performance_chart.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
