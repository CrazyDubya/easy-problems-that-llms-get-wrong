{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import requests\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import litellm\n",
    "import os\n",
    "from decouple import AutoConfig\n",
    "\n",
    "config = AutoConfig(search_path='.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set ENV variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = config('OPENAI_API_KEY')\n",
    "os.environ[\"COHERE_API_KEY\"] = config('COHERE_API_KEY')\n",
    "os.environ[\"MISTRAL_API_KEY\"] = config('MISTRAL_API_KEY')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = config('ANTHROPIC_API_KEY')\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config('AWS_ACCESS_KEY_ID')\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config('AWS_SECRET_ACCESS_KEY')\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"us-west-2\"\n",
    "litellm.vertex_project = config('VERTEX_PROJECT')\n",
    "litellm.vertex_location = config('VERTEX_LOCATION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test LLM Service \n",
    "\n",
    "#messages = [{ \"content\": \"Write a sentence where every word starts with the letter A.\",\"role\": \"user\"}]\n",
    "\n",
    "#response = litellm.completion(model=\"gpt-4-turbo-preview\", messages=messages, temperature=1, max_tokens=50)\n",
    "#response = litellm.completion(model=\"meta.llama3-70b-instruct-v1:0\", messages=messages, temperature=0, max_tokens=50)\n",
    "#response = litellm.completion(model=\"command-r\", messages=messages, temperature=0, max_tokens=50)\n",
    "#response = litellm.completion(model=\"mistral/mistral-large-latest\", messages=messages, temperature=0, max_tokens=50)\n",
    "#response = litellm.completion(model=\"mistral/open-mixtral-8x22b\", messages=messages)\n",
    "#response = litellm.completion(model=\"claude-3-opus-20240229\", messages=messages)\n",
    "#response = litellm.completion(model=\"gemini-1.5-pro\", messages=messages)\n",
    "#response = litellm.completion(model=\"gemini-pro\", messages=messages)\n",
    "\n",
    "#message_parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_parse(response):\n",
    "    messages = [m['message']['content'] for m in response['choices']]\n",
    "    if len(messages) == 1:\n",
    "        messages = messages[0]\n",
    "    return messages\n",
    "\n",
    "\n",
    "\n",
    "def openai_query(messages:list, model='gpt-4-turbo-preview', max_tokens=1000, temperature=0, n=1):\n",
    "    \"\"\"\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the meaning of life?\"\n",
    "        },\n",
    "    ]\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\",\n",
    "        headers={  \n",
    "            \"Authorization\": f\"Bearer {config('OPENAI_API_KEY')}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"n\": n,\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# response = openai_query(\n",
    "#     messages=[{\"role\": \"user\",\"content\": \"What is the meaning of life?\"}],\n",
    "#     max_tokens=10,\n",
    "#     temperature=0,\n",
    "#     n=1,\n",
    "# )\n",
    "\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_in_executor(func, *args, **kwargs):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        result = await loop.run_in_executor(pool, lambda: func(*args, **kwargs))\n",
    "    return result\n",
    "\n",
    "\n",
    "async def runner(func, prompts, batch_size=1, **kwargs):\n",
    "    all_responses = []\n",
    "    for idx in range(0, len(prompts), batch_size):\n",
    "        print(f\"Processing batch {idx + 1}-{idx + batch_size} ex {len(prompts)}\")\n",
    "        batch_prompts = prompts[idx: idx + batch_size]\n",
    "        responses = await asyncio.gather(*(run_in_executor(func, messages=prompt, **kwargs) for prompt in batch_prompts))\n",
    "        all_responses.extend(responses)\n",
    "    return all_responses\n",
    "\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"What is the meaning of life?\"}]\n",
    "\n",
    "# responses = await runner(litellm.completion, prompts=[messages] * 2, batch_size=5, \n",
    "#                          model=\"claude-3-opus-20240229\", temperature=1, max_tokens=50)\n",
    "\n",
    "# for response in responses:\n",
    "#     print(message_parse(response))\n",
    "#     print('\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_questions = json.load(open('linguistic_benchmark.json', 'r'))\n",
    "benchmark_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [[{\"role\": \"user\", \"content\": q['question']}] for q in benchmark_questions]\n",
    "# #messages = [{\"role\": \"user\", \"content\": benchmark_questions[0]['question']}]\n",
    "# messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = await runner(litellm.completion, prompts=messages, batch_size=10, \n",
    "                       model=\"mistral/open-mixtral-8x22b\", temperature=0, max_tokens=2048)\n",
    "\n",
    "for answer in answers:\n",
    "    print(message_parse(answer))\n",
    "    print('\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the benchmark questions\n",
    "final_answers = copy.deepcopy(benchmark_questions)\n",
    "\n",
    "# Join the eval responses and scores with the benchmark questions\n",
    "for idx, question in enumerate(final_answers):\n",
    "    question.update({'model_answer': message_parse(answers[idx])})\n",
    "    question.update({'score': ''})\n",
    "\n",
    "answers_df = pd.DataFrame(final_answers)\n",
    "\n",
    "answers_df.set_index('index').to_json('./llm_outputs/final_answers-mistral.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_outputs = {}\n",
    "for output_file in os.listdir(\"./llm_outputs/\"):\n",
    "    if output_file.endswith(\".json\"):\n",
    "        outputs_df = pd.read_json(f\"./llm_outputs/{output_file}\", orient='index')\n",
    "        model = output_file.replace('final_answers-', '').replace('.json', '')\n",
    "        all_model_outputs[model] = outputs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_stats = {}\n",
    "for model, outputs in all_model_outputs.items():\n",
    "    print(f\"Calculating stats for {model}\")\n",
    "    mean_score = outputs['score'].mean()\n",
    "    std_dev_score = outputs['score'].std()\n",
    "    # do a 10,000 bootstrap to get the 95% CI\n",
    "    bootstrap_scores = []\n",
    "    for _ in range(10000):\n",
    "        bootstrap_scores.append(outputs['score'].sample(frac=1, replace=True).mean())\n",
    "    ci_lower = np.percentile(bootstrap_scores, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_scores, 97.5)\n",
    "    # caculate z-interval 95%\n",
    "    z = 1.96\n",
    "    z_interval_error = z * (std_dev_score / np.sqrt(len(outputs)))\n",
    "    all_model_stats[model] = {\n",
    "        'mean_score': mean_score, \n",
    "        'std_dev_score': std_dev_score, \n",
    "        'z_interval_error': z_interval_error, \n",
    "        'ci_lower': ci_lower, \n",
    "        'ci_upper': ci_upper,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(all_model_stats).transpose().sort_values('mean_score', ascending=False).round(0)\n",
    "stats_df.index.name = 'model'\n",
    "\n",
    "#stats_df.to_csv('./tables_and_charts/final_stats.csv')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evlauation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(question, human_response, model_response):\n",
    "    return f\"\"\"SCORING CRITERIA\n",
    "100%: The response contains the correct answer only with a correct thought process and no logical inconsistencies.\n",
    "80%: The response contains the correct answer only with a correct thought process with some logical inconsistencies.\n",
    "60%: The response contains the correct answer only but with an incorrect thought process.\n",
    "40%: The response contains an incorrect answer anywhere but also provides a cor-rect answer or correct thought process with minimal logical inconsistencies.\n",
    "20%: The response contains an incorrect answer anywhere but provides enough helpful information to plausibly reach a correct answer.\n",
    "0% The response contains an incorrect answer, too much unhelpful information, or not enough helpful information to plausibly reach a correct answer\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "PERFECT RESPONSE\n",
    "{human_response}\n",
    "\n",
    "STUDENT RESPONSE\n",
    "{model_response} \n",
    "\n",
    "TASK\n",
    "Does the STUDENT RESPONSE cointain the PERFECT RESPONSE only! Use the SCORING CRITERIA. Provide a full explanation and finally return a JSON object with the score as a percentage. Example:\n",
    "{{\"score\": 40}}\n",
    "\"\"\"\n",
    "\n",
    "print(create_eval_prompt(question='#', human_response='#', model_response='#'))\n",
    "\n",
    "\n",
    "def extract_valid_json(s):\n",
    "    # Regex pattern for basic JSON structure: objects {} and arrays []\n",
    "    json_pattern = re.compile(r'\\{.*?\\}|\\[.*?\\]', re.DOTALL)\n",
    "    # Finding all matches that look like JSON\n",
    "    potential_jsons = json_pattern.findall(s)\n",
    "\n",
    "    for pj in potential_jsons:\n",
    "        pj = pj.replace('%', '')\n",
    "        try:\n",
    "            # Attempt to parse the JSON\n",
    "            valid_json = json.loads(pj)\n",
    "            # Returning the first valid JSON found\n",
    "            return valid_json\n",
    "        except json.JSONDecodeError:\n",
    "            # If it's not valid JSON, move on to the next match\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_model_outputs.keys())\n",
    "\n",
    "model = 'mistral-8x22b'\n",
    "answers = all_model_outputs[model]['model_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_prompts = []\n",
    "for idx, question in enumerate(benchmark_questions):\n",
    "    eval_prompt = create_eval_prompt(\n",
    "        question=question['question'],\n",
    "        human_response=question['human_answer'],\n",
    "        model_response=answers[idx],\n",
    "    )\n",
    "    all_eval_prompts.append(eval_prompt)\n",
    "\n",
    "# all_eval_prompts\n",
    "eval_messages = [[{\"role\": \"user\", \"content\": p}] for p in all_eval_prompts]\n",
    "eval_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_responses = await runner(openai_query, prompts=eval_messages, batch_size=30, temperature=0, max_tokens=4000)\n",
    "\n",
    "for eval in eval_responses:\n",
    "    print(eval)\n",
    "    print(message_parse(eval))\n",
    "    print('\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_scores = []\n",
    "for eval in eval_responses:\n",
    "    eval_response = message_parse(eval)\n",
    "    score_json = extract_valid_json(eval_response)\n",
    "    if not isinstance(score_json['score'], int):\n",
    "        score_json['score'] = np.nan\n",
    "        print('Score not an integer, setting to 0.')\n",
    "    all_scores.append(score_json)\n",
    "\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = copy.deepcopy(benchmark_questions)\n",
    "\n",
    "# Join the eval responses and scores with the benchmark questions\n",
    "for idx, question in enumerate(final_results):\n",
    "    question.update({'model_answer': answers[idx]})\n",
    "    question.update({'eval_response': message_parse(eval_responses[idx])})\n",
    "    question.update(all_scores[idx])\n",
    "\n",
    "final_df = pd.DataFrame(final_results)\n",
    "print(final_df['score'].mean(), final_df['score'].std())\n",
    "\n",
    "final_df.set_index('index').to_json(f'./automated_evals/auto_eval-{model}.json', orient='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
