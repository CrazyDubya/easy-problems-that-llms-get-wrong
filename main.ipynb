{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from charting import create_performance_chart\n",
    "from llm_service import litellm_service, custom_llm_service\n",
    "from utils import get_llm_answers, get_llm_stats, load_all_llm_answers_from_json, model_clean\n",
    "from auto_eval import (\n",
    "    create_all_llm_eval_messages, \n",
    "    extract_all_scores, \n",
    "    create_auto_eval_json, \n",
    "    get_llm_eval_responses, \n",
    "    score_multiple_choice_answers,\n",
    "    validation_func,\n",
    ")\n",
    "from multiple_choice import construct_multiple_choice_question\n",
    "\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_models = [\n",
    "    \"gpt-4-turbo-preview\", \n",
    "    \"gpt-4o\",\n",
    "    \"meta.llama3-70b-instruct-v1:0\", \n",
    "    \"mistral/mistral-large-latest\", \n",
    "    \"mistral/open-mixtral-8x22b\", \n",
    "    \"claude-3-opus-20240229\", \n",
    "    \"vertex_ai/gemini-1.5-pro\", \n",
    "    \"vertex_ai/gemini-1.0-pro\",\n",
    "    \"command-r\", \n",
    "]\n",
    "\n",
    "answer_rounds = 10 # Number of rounds of questions to ask each model\n",
    "answer_hyperparams = {\n",
    "    'batch_size': 10, # Max number of questions to send to a model at once (10 is sensible)\n",
    "    'temperature': 0, # 0 is default and the most deterministic\n",
    "    'max_tokens': 2048, # 2048 works for most models, but may have to be reduced for some models\n",
    "    'num_retries': 5, # Number of times to retry a question if it fails\n",
    "}\n",
    "\n",
    "auto_eval_rounds = 1 # Number of rounds of auto evaluation to run to then average the scores\n",
    "auto_eval_model = \"gpt-4o\"\n",
    "auto_eval_hyperparams= {\n",
    "    'temperature': 0,\n",
    "    'max_tokens': 2048,\n",
    "    'batch_size': 30,\n",
    "}\n",
    "\n",
    "multiple_choice_questions = True\n",
    "benchmark_name = 'Benchmark' if not multiple_choice_questions else 'Multi-Benchmark'\n",
    "date_now = datetime.now().strftime('%Y-%m-%d') #'2024-06-10'\n",
    "answers_save_path = f\"./{date_now}-{benchmark_name}/llm_outputs\"\n",
    "auto_eval_save_path = f\"./{date_now}-{benchmark_name}/auto_eval_outputs\"\n",
    "stats_save_path = f\"./{date_now}-{benchmark_name}/tables_and_charts\"\n",
    "\n",
    "\n",
    "execution_steps = [\n",
    "    \"get_llm_answers\",\n",
    "    \"auto_evaluate_answers\",\n",
    "    \"generate_statistics\", \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_incomplete_llm_answers(all_llm_answers, auto_eval_save_path, sub_eval_folders, date_now):\n",
    "    all_llm_evals = load_all_llm_answers_from_json(auto_eval_save_path,\n",
    "        prefix_replace='auto_eval-', sub_folders=sub_eval_folders)\n",
    "    skip_evals = set(all_llm_evals.keys() & set(all_llm_answers.keys()))\n",
    "    print(f'Skipping existing LLM evals (in {date_now} folder):', skip_evals)\n",
    "    incomplete_llm_answers = {model: value for model, value in all_llm_answers.items() \n",
    "                               if model_clean(model) not in skip_evals}\n",
    "    return incomplete_llm_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in benchmark questions\n",
    "if multiple_choice_questions:\n",
    "    benchmark_questions = json.load(open('linguistic_benchmark_multi_choice.json', 'r'))\n",
    "    sub_eval_folders = ['']\n",
    "    def answer_validation_func(x):\n",
    "        return validation_func(x, json_key='ANSWER', list_of_values=['A', 'B', 'C', 'D'])\n",
    "else:\n",
    "    benchmark_questions = json.load(open('linguistic_benchmark.json', 'r'))\n",
    "    sub_eval_folders = [f'/round_{r+1}' for r in range(auto_eval_rounds)]\n",
    "sub_answer_folders = [f'/round_{r+1}' for r in range(answer_rounds)]\n",
    "\n",
    "\n",
    "if \"get_llm_answers\" in execution_steps:\n",
    "    print('1. GETTING LLM ANSWERS')\n",
    "    # Load in any existing answers and evals to avoid overwriting them\n",
    "    for n in range(answer_rounds):\n",
    "        print(f'\\n----- Round: {n+1} of {answer_rounds} -----')\n",
    "        answer_save_path_round = f\"{answers_save_path}/round_{n+1}\"\n",
    "        all_llm_answers = load_all_llm_answers_from_json(answer_save_path_round,\n",
    "            prefix_replace='final_answers-')\n",
    "        print(f'Skipping existing LLM answers (in {date_now} folder):', list(all_llm_answers.keys()))\n",
    "        answer_models_run = [model for model in answer_models \n",
    "                            if model_clean(model) not in all_llm_answers.keys()]\n",
    "        if multiple_choice_questions:\n",
    "            for q in benchmark_questions:\n",
    "                prompt, correct_letter = construct_multiple_choice_question(q)\n",
    "                q.update({'multi_choice_question': prompt, 'correct_letter': correct_letter})\n",
    "        all_llm_answers = await get_llm_answers(\n",
    "            litellm_service(),\n",
    "            benchmark_questions,\n",
    "            answer_models_run,\n",
    "            answer_hyperparams,\n",
    "            answer_save_path_round,\n",
    "            multiple_choice_questions,\n",
    "            validation_func=answer_validation_func if multiple_choice_questions else lambda x: True\n",
    "        )\n",
    "    print('-- DONE ANSWERS --\\n')\n",
    "\n",
    "\n",
    "if \"auto_evaluate_answers\" in execution_steps:\n",
    "    print('2. AUTO EVALUATING ANSWERS')\n",
    "    all_llm_answers = load_all_llm_answers_from_json(answers_save_path,\n",
    "        prefix_replace='final_answers-', sub_folders=sub_answer_folders)\n",
    "    if multiple_choice_questions:\n",
    "        incomplete_llm_answers = calc_incomplete_llm_answers(all_llm_answers, auto_eval_save_path,\n",
    "                                                             sub_eval_folders, date_now)\n",
    "        all_llm_evals = score_multiple_choice_answers(incomplete_llm_answers, auto_eval_save_path)\n",
    "    else:\n",
    "        all_llm_eval_messages = create_all_llm_eval_messages(all_llm_answers, benchmark_questions)\n",
    "        for n in range(auto_eval_rounds):\n",
    "            print(f'- Round: {n+1} of {auto_eval_rounds} -')\n",
    "            incomplete_llm_answers = calc_incomplete_llm_answers(all_llm_eval_messages, auto_eval_save_path,\n",
    "                                                                 [f'/round_{n+1}'], date_now)\n",
    "            all_llm_eval_responses = await get_llm_eval_responses(\n",
    "                custom_llm_service(), \n",
    "                incomplete_llm_answers,\n",
    "                model=auto_eval_model, \n",
    "                hyperparams=auto_eval_hyperparams,\n",
    "                validation_func=lambda x: validation_func(x, json_key='score', \n",
    "                    list_of_values=[0, 20, 40, 60, 80, 100])\n",
    "            )\n",
    "            all_llm_scores = extract_all_scores(all_llm_eval_responses)\n",
    "            auto_eval_save_path_n = f\"{auto_eval_save_path}/round_{n+1}\"\n",
    "            all_auto_results = create_auto_eval_json(\n",
    "                all_llm_scores, \n",
    "                all_llm_eval_responses, \n",
    "                all_llm_answers, \n",
    "                benchmark_questions, \n",
    "                auto_eval_save_path_n\n",
    "            )\n",
    "    print('-- DONE AUTO EVAL --\\n')\n",
    "\n",
    "\n",
    "if \"generate_statistics\" in execution_steps:\n",
    "    print('3. GENERATING STATISTICS')\n",
    "    all_llm_evals = load_all_llm_answers_from_json(\n",
    "        auto_eval_save_path, \n",
    "        prefix_replace='auto_eval-',\n",
    "        sub_folders=sub_eval_folders,\n",
    "    )\n",
    "    stats_df = get_llm_stats(all_llm_evals, stats_save_path, bootstrap_n=10000)\n",
    "    if multiple_choice_questions:\n",
    "        for model, evals_df in all_llm_evals.items():\n",
    "            #evals_df['invalid_answer_letter'] = evals_df.apply(lambda x: x['json_answer_letter'] not in ['A', 'B', 'C', 'D'], axis=1)\n",
    "            incorrect_letter_count = evals_df['invalid_answer_letter'].sum()\n",
    "            print(model, incorrect_letter_count)\n",
    "            stats_df.loc[model, 'invalid_outputs'] = incorrect_letter_count\n",
    "\n",
    "    display(stats_df)\n",
    "    barplot, plt = create_performance_chart(stats_df.reset_index())\n",
    "    barplot.figure.savefig(f\"{stats_save_path}/performance_chart.png\")\n",
    "    plt.show()\n",
    "    print('-- DONE STATS --\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = all_llm_evals['gpt-4-turbo-preview'][['question', 'model_answer', 'json_answer', 'correct_letter', 'score']]\n",
    "for row in answers.to_dict('records'):\n",
    "    print('--------------')\n",
    "    print('\\nQuestion\\n', row['question'])\n",
    "    print('\\nModel Answer\\n', row['model_answer'])\n",
    "    print('\\nJson Answer\\n', row['json_answer'])\n",
    "    print('\\nCorrect Letter\\n', row['correct_letter'])    \n",
    "    print('\\nScore\\n', row['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Auto Eval Consistancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_llm_evals = load_all_llm_answers_from_json(\n",
    "    auto_eval_save_path, \n",
    "    prefix_replace='auto_eval-',\n",
    "    sub_folders=sub_eval_folders,\n",
    ")\n",
    "models = list(all_llm_evals.keys())\n",
    "\n",
    "\n",
    "model = models[3]\n",
    "print(f\"Model: {model}\")\n",
    "auto_eval_agg = all_llm_evals[model].set_index('level_0').groupby('index').agg({'score': ['mean', 'min', 'max']})\n",
    "auto_eval_agg.index.name = 'Question #'\n",
    "auto_eval_agg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
